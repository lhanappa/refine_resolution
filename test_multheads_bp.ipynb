{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit648325705e4b41c8bbabe44d0e358718",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cooler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from iced import normalization\n",
    "import spektral\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from ftp://cooler.csail.mit.edu/coolers/hg19/\n",
    "name = 'Dixon2012-H1hESC-HindIII-allreps-filtered.100kb.cool'\n",
    "#name = 'Rao2014-K562-MboI-allreps-filtered.500kb.cool'\n",
    "c = cooler.Cooler(name)\n",
    "resolution = c.binsize\n",
    "mat= c.matrix(balance=True).fetch('chr2')\n",
    "print(mat.shape)\n",
    "idxy = ~np.all(np.isnan(mat),axis=0)\n",
    "M = mat[idxy,:]\n",
    "Mh = M[:,idxy]\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = int(Mh.shape[0]/4),int(Mh.shape[1]/4)\n",
    "img_l = np.zeros(shape=(IMG_HEIGHT, IMG_WIDTH))\n",
    "for i in list(range(0, len(Mh))):\n",
    "    x = int(np.floor(i/(len(Mh)/IMG_HEIGHT)))\n",
    "    for j in list(range(0,len(Mh))):\n",
    "        y = int(np.floor(j/(len(Mh)/IMG_WIDTH)))\n",
    "        img_l[x, y] = img_l[x, y] + Mh[i,j]\n",
    "\n",
    "Ml = img_l\n",
    "plt.matshow(np.log2(Ml), cmap='YlOrRd')\n",
    "print(Ml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "Ml = img_l\n",
    "#print('original Ml: ', Ml)\n",
    "#Ml = normalization.ICE_normalization(Ml)\n",
    "Ml = normalization.SCN_normalization(Ml)\n",
    "Mh = normalization.SCN_normalization(Mh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.log2(Ml))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.log2(Mh))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hic_lr = []\n",
    "IMG_HEIGHT, IMG_WIDTH = int(512/4),int(512/4)\n",
    "print(IMG_HEIGHT, IMG_WIDTH)\n",
    "for i in range(len(Ml)-IMG_HEIGHT+1):\n",
    "    hic_lr.append(Ml[i:i+IMG_HEIGHT, i:i+IMG_WIDTH])\n",
    "hic_lr = np.array(hic_lr)\n",
    "print(hic_lr.shape)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(np.log2(hic_lr[i*25,:,:]), cmap='YlOrRd')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hic_hr = []\n",
    "IMG_HEIGHT, IMG_WIDTH = int(512),int(512)\n",
    "print(IMG_HEIGHT, IMG_WIDTH)\n",
    "for i in range(0,len(Mh)-IMG_HEIGHT+1,4):\n",
    "    hic_hr.append(Mh[i:i+IMG_HEIGHT, i:i+IMG_WIDTH])\n",
    "hic_hr = np.array(hic_hr)\n",
    "print(hic_hr.shape)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(np.log2(hic_hr[i*25,:,:]), cmap='YlOrRd')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "  result.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "  return result\n",
    "\n",
    "def make_discriminator_model():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[512, 512, 1], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[512, 512, 1], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar])\n",
    "\n",
    "  down1 = downsample(64, 4, False)(x) \n",
    "  down2 = downsample(128, 4)(down1)\n",
    "  #down3 = downsample(256, 4)(down2)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down2)\n",
    "  conv = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)\n",
    "\n",
    "  '''batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)'''\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconstruct_R1M(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, name='RR'):\n",
    "        super(Reconstruct_R1M, self).__init__(name=name)\n",
    "        self.num_outputs = filters\n",
    "        w_init = tf.ones_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(1,1,1,filters), dtype='float32'))\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "        \n",
    "    def call(self, input):\n",
    "        v = tf.math.add(input, tf.constant(1e-6, dtype=tf.float32))\n",
    "        vt = tf.transpose(v, perm=[0,2,1,3])\n",
    "        rank1m = tf.multiply(tf.multiply(v, vt), self.w)\n",
    "        return rank1m\n",
    "\n",
    "class Weight_R1M(tf.keras.layers.Layer):\n",
    "    def __init__(self, name='WR1M'):\n",
    "        super(Weight_R1M, self).__init__(name=name)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.keras.initializers.RandomUniform(minval=0, maxval=4.0)\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(1,1,1,input_shape[-1]), dtype='float32'))\n",
    "    \n",
    "    def call(self, input):\n",
    "        self.w.assign(tf.nn.relu(self.w))\n",
    "        return tf.multiply(input, self.w)\n",
    "\n",
    "class Sum_R1M(tf.keras.layers.Layer):\n",
    "    def __init__(self, name='SR1M'):\n",
    "        super(Sum_R1M, self).__init__(name=name)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, input):\n",
    "        return tf.reduce_sum(input, axis=-1, keepdims=True)\n",
    "\n",
    "class Normal(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, name='DW'):\n",
    "        super(Normal, self).__init__(name=name)\n",
    "        w_init = tf.ones_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(1, input_dim, 1, 1), dtype='float32'), trainable=True)\n",
    "        d_init = tf.zeros_initializer()\n",
    "        self.d = tf.Variable(initial_value=d_init(shape=(1, input_dim), dtype='float32'), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        rowsr = tf.math.sqrt(tf.math.reduce_sum(tf.multiply(inputs, inputs), axis=1, keepdims=True))\n",
    "        colsr = tf.math.sqrt(tf.math.reduce_sum(tf.multiply(inputs, inputs), axis=2, keepdims=True))\n",
    "        sumele = tf.math.multiply(rowsr, colsr)\n",
    "        #tf.math.divide_no_nan(inputs, sumele)\n",
    "        Div = tf.math.divide_no_nan(inputs, sumele)\n",
    "        self.w.assign(tf.nn.relu(self.w))\n",
    "        self.d.assign(tf.nn.relu(self.d))\n",
    "        WT = tf.transpose(self.w, perm=[0,2,1,3])\n",
    "        M = tf.multiply(self.w, WT)\n",
    "        opd = tf.linalg.LinearOperatorToeplitz(self.d, self.d)\n",
    "        opd = tf.expand_dims(opd.to_dense(), axis=-1)\n",
    "        return tf.add(tf.multiply(Div, M), opd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    In = tf.keras.layers.Input(shape=(128, 128, 1), name='In', dtype=tf.float32)\n",
    "    Decl = tf.keras.layers.Conv2D(32, [1, 128], strides=1, padding='valid', data_format=\"channels_last\", activation='relu', use_bias=False, kernel_constraint=tf.keras.constraints.NonNeg(),kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.01, stddev=0.1),  name='Decl')(In) \n",
    "    \n",
    "    WeiR1Ml = Weight_R1M(name='WR1Ml')(Decl)\n",
    "    Recl = Reconstruct_R1M(32, name='Recl')(WeiR1Ml)\n",
    "    Suml = Sum_R1M(name='Suml')(Recl) \n",
    "    low_out = Normal(128, name='Out_low')(Suml)\n",
    "    \n",
    "    up_o = tf.keras.layers.UpSampling2D(size=(4,4), data_format='channels_last', name='Upo')(In)\n",
    "    m_F = tf.constant(1/16.0, shape=(1,1,1,1))\n",
    "    up_o = tf.keras.layers.Multiply()([up_o, m_F])\n",
    "\n",
    "    up_1 = tf.keras.layers.UpSampling2D(size=(4,1), data_format='channels_last', name='UpSample')(WeiR1Ml)\n",
    "    m_F = tf.constant(1/4.0, shape=(1,1,1,1))\n",
    "    up_1 = tf.keras.layers.Multiply()([up_1, m_F])\n",
    "    #trans_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(17,1), strides=(1,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=True, name='C2DT1')(up_1)\n",
    "    #trans_2 = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(5,1), strides=(2,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=False, kernel_constraint=tf.keras.constraints.NonNeg(), name='C2DT2')(trans_1)\n",
    "    Rech = Reconstruct_R1M(32, name='Rech')(up_1)\n",
    "    #DepthwiseConv2D or SeparableConv2D https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/\n",
    "    trans_1_1 = tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3), strides=(1,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=True, name='C2DT1_1')(Rech)\n",
    "    trans_1_2 = tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3), strides=(1,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=True, name='C2DT1_2')(Rech)\n",
    "    #trans_2_1 = tf.keras.layers.DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=True, name='C2DT2_1')(Rech)\n",
    "    #trans_2_2 = tf.keras.layers.DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=True, name='C2DT2_2')(Rech)\n",
    "    #trans_3_1 = tf.keras.layers.DepthwiseConv2D(kernel_size=(7,7), strides=(1,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=True, name='C2DT3_1')(Rech)\n",
    "    #trans_3_2 = tf.keras.layers.DepthwiseConv2D(kernel_size=(7,7), strides=(1,1), padding='same', data_format=\"channels_last\", activation='relu', use_bias=True, name='C2DT3_2')(Rech)\n",
    "    Concat = tf.keras.layers.concatenate([trans_1_1, trans_1_2])#, trans_2_1, trans_2_2, trans_3_1, trans_3_2])\n",
    "    WeiR1Mh = Weight_R1M(name='WR1Mh')(Concat)\n",
    "    Sumh = Sum_R1M(name='Sumh')(WeiR1Mh) \n",
    "    high_out = Normal(512, name='Out_high')(Sumh)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[In],outputs=[low_out, high_out, up_o])\n",
    "    return model\n",
    "\n",
    "'''p1,p2,heads, gen = make_generator_model()\n",
    "p1.summary()\n",
    "p2.summary()\n",
    "heads[1].summary()\n",
    "gen.summary()\n",
    "print(gen.get_layer('Dec1').trainable_variables)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gen_low = make_generator_low_model()\n",
    "#Gen_high = make_generator_high_model()\n",
    "Gen = make_generator_model()\n",
    "Dis = make_discriminator_model()\n",
    "print(Gen.summary())\n",
    "tf.keras.utils.plot_model(Gen, to_file='G.png',show_shapes=True)\n",
    "'''print(p1.summary())\n",
    "tf.keras.utils.plot_model(Gen_high, to_file='G_low.png',show_shapes=True)\n",
    "print(p2.summary())\n",
    "tf.keras.utils.plot_model(Gen_high, to_file='G_high.png',show_shapes=True)'''\n",
    "print(Dis.summary())\n",
    "tf.keras.utils.plot_model(Dis, to_file='D.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = loss_object(tf.ones_like(real_output), real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(fake_output), fake_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "    \n",
    "def generator_lowr_loss(y_pred, y_true):\n",
    "    #return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    return 1 - tf.image.ssim(y_pred, y_true, max_val=1.0)\n",
    "\n",
    "def generator_highr_loss(d_pred):\n",
    "    gan_loss = loss_object(tf.ones_like(d_pred), d_pred)\n",
    "    return gan_loss\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer_low = tf.keras.optimizers.Adam()\n",
    "generator_optimizer_high = tf.keras.optimizers.Adam()\n",
    "discriminator_optimizer = tf.keras.optimizers.Adagrad()\n",
    "'''checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=Gen,\n",
    "                                 discriminator=Dis)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(imgl, imgr):\n",
    "    with tf.GradientTape() as gen_tape_low, tf.GradientTape() as gen_tape_high, tf.GradientTape() as disc_tape:\n",
    "        fake_hic = Gen(imgl, training=True)\n",
    "        fake_hic_l = fake_hic[0]\n",
    "        fake_hic_h = fake_hic[1]\n",
    "        img_l_h = fake_hic[2]\n",
    "        gen_loss_low = generator_lowr_loss(fake_hic_l, imgl)\n",
    "        #gen_low_v = Gen.trainable_variables\n",
    "        gen_low_v = []\n",
    "        gen_low_v += Gen.get_layer('Decl').trainable_variables\n",
    "        gen_low_v += Gen.get_layer('WR1Ml').trainable_variables\n",
    "        gen_low_v += Gen.get_layer('Recl').trainable_variables        \n",
    "        gen_low_v += Gen.get_layer('Out_low').trainable_variables\n",
    "        gradients_of_generator_low = gen_tape_low.gradient(gen_loss_low, gen_low_v)\n",
    "        generator_optimizer_low.apply_gradients(zip(gradients_of_generator_low, gen_low_v))\n",
    "\n",
    "        #disc_generated_output = Dis([fake_hic_h, imgr], training=True)\n",
    "\n",
    "        gen_loss_high = generator_lowr_loss(fake_hic_h, imgr) #+ generator_highr_loss(disc_generated_output)\n",
    "        gen_high_v = []\n",
    "        gen_high_v += Gen.get_layer('Rech').trainable_variables\n",
    "        gen_high_v += Gen.get_layer('C2DT1_1').trainable_variables\n",
    "        gen_high_v += Gen.get_layer('C2DT1_2').trainable_variables\n",
    "        #gen_high_v += Gen.get_layer('C2DT2_1').trainable_variables\n",
    "        #gen_high_v += Gen.get_layer('C2DT2_2').trainable_variables\n",
    "        #gen_high_v += Gen.get_layer('C2DT3_1').trainable_variables\n",
    "        #gen_high_v += Gen.get_layer('C2DT3_2').trainable_variables\n",
    "        gen_high_v += Gen.get_layer('WR1Mh').trainable_variables\n",
    "        gen_high_v += Gen.get_layer('Out_high').trainable_variables\n",
    "        #gen_high_v += Gen.get_layer('C2DT2').trainable_variables\n",
    "        #gen_high_v += Gen.get_layer('Rech').trainable_variables\n",
    "        gradients_of_generator_high = gen_tape_high.gradient(gen_loss_high, gen_high_v)\n",
    "        generator_optimizer_high.apply_gradients(zip(gradients_of_generator_high, gen_high_v))\n",
    "\n",
    "        '''disc_real_output = Dis([img_l_h, imgr], training=True)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss, Dis.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, Dis.trainable_variables))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, BATCH_SIZE):\n",
    "    #print(dataset)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for i, (low_m, high_m) in enumerate(dataset.take(1)):\n",
    "            train_step(tf.dtypes.cast(low_m, tf.float32), tf.dtypes.cast(high_m, tf.float32))\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            # Produce images for the GIF as we go\n",
    "            display.clear_output(wait=True)\n",
    "            generate_and_save_images(Gen, epoch + 1, None)\n",
    "            #checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        print ('Time for epoch {} is {} sec.'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    #display.clear_output(wait=True)\n",
    "    #generate_and_save_images(Gen, epochs, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    test_input = np.ones(shape=[1,128,128,1])\n",
    "    test_input[0,:,:,0] = hic_lr[0,:,:]\n",
    "    pre = model(test_input, training=False)\n",
    "    predictions = np.squeeze(pre[0])\n",
    "    #print('Pred: ', predictions)\n",
    "    rows = 3\n",
    "    cols = 4\n",
    "    plt.figure(figsize=(6, 6), constrained_layout=True)\n",
    "    plt.subplot(rows,cols,1)\n",
    "    plt.imshow(np.log2(np.squeeze(hic_lr[0,:,:])))\n",
    "    #plt.axis('off')\n",
    "    plt.title('Original')\n",
    "    plt.subplot(rows,cols,2)\n",
    "    plt.imshow(np.log2(predictions))\n",
    "    #plt.axis('off')\n",
    "    plt.title('X Hat')\n",
    "    plt.subplot(rows,cols,3)\n",
    "    plt.imshow(np.log2(np.squeeze(hic_lr[0,:,:])-predictions), cmap='RdBu_r')\n",
    "    #plt.axis('off')\n",
    "    plt.title('Diff')\n",
    "\n",
    "    intermediate_layer_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer('Suml').output )\n",
    "    intermediate_output = intermediate_layer_model.predict(test_input)\n",
    "    plt.subplot(rows,cols,4)\n",
    "    plt.imshow(np.log2(np.squeeze(intermediate_output)), cmap='RdBu_r')\n",
    "    #plt.axis('off')\n",
    "    plt.title('before WX+b')\n",
    "    m = model.get_layer('Out_high').get_weights()\n",
    "    plt.subplot(rows,cols,5)\n",
    "    plt.plot(np.squeeze(m[0]))\n",
    "    plt.title('W')\n",
    "    plt.subplot(rows,cols,6)\n",
    "    plt.plot(np.squeeze(m[1]))\n",
    "    plt.title('bias')\n",
    "\n",
    "    intermediate_layer_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer('Sumh').output )\n",
    "    intermediate_output = np.squeeze(pre[1])\n",
    "    plt.subplot(rows,cols,7)\n",
    "    plt.imshow(np.log2(np.squeeze(intermediate_output)), cmap='RdBu_r')\n",
    "    #plt.axis('off')\n",
    "    plt.title('HX Hat')\n",
    "\n",
    "    intermediate_layer_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer('Decl').output )\n",
    "    intermediate_output = intermediate_layer_model.predict(test_input)\n",
    "    plt.subplot(rows,cols,8)\n",
    "    plt.imshow(np.squeeze(intermediate_output))\n",
    "    plt.axis('off')\n",
    "    plt.title('Decl')\n",
    "    intermediate_layer_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer('WR1Ml').output )\n",
    "    intermediate_output = intermediate_layer_model.predict(test_input)\n",
    "    plt.subplot(rows,cols,9)\n",
    "    plt.imshow(np.squeeze(intermediate_output))\n",
    "    plt.axis('off')\n",
    "    plt.title('WR1Ml')\n",
    "\n",
    "    intermediate_layer_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer('UpSample').output )\n",
    "    intermediate_output = intermediate_layer_model.predict(test_input)\n",
    "    plt.subplot(rows,cols,10)\n",
    "    plt.imshow(np.squeeze(intermediate_output))\n",
    "    plt.axis('off')\n",
    "    plt.title('UpSample')\n",
    "\n",
    "    '''intermediate_layer_model = tf.keras.models.Model(inputs=model.input, outputs=model.get_layer('C2DT1').output )\n",
    "    intermediate_output = intermediate_layer_model.predict(test_input)\n",
    "    plt.subplot(rows,cols,11)\n",
    "    plt.imshow(np.squeeze(intermediate_output))\n",
    "    plt.axis('off')\n",
    "    plt.title('C2DT1')'''\n",
    "\n",
    "    plt.savefig('./lvl3/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "EPOCHS = 800\n",
    "BUFFER_SIZE = 1\n",
    "BATCH_SIZE = 3\n",
    "train_data = tf.data.Dataset.from_tensor_slices((hic_lr[..., np.newaxis], hic_hr[..., np.newaxis])).batch(BATCH_SIZE)\n",
    "#train_low = tf.data.Dataset.from_tensor_slices(hic_lr[..., np.newaxis]).batch(BATCH_SIZE)\n",
    "#train_high = tf.data.Dataset.from_tensor_slices(hic_hr[..., np.newaxis]).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "train(train_data, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iced\n",
    "import copy\n",
    "idx = 0\n",
    "test_input = np.ones(shape=[1,128,128,1])\n",
    "test_input[0,:,:,0] = copy.copy(hic_lr[idx,:,:])\n",
    "predictions = Gen(test_input, training=False)\n",
    "hic_pred = np.squeeze((predictions[1]).numpy())\n",
    "#hic_pred = normalization.SCN_normalization(hic_pred)\n",
    "plt.figure(figsize=(8,24))\n",
    "plt.subplot(1,4,1)\n",
    "T = np.squeeze(hic_hr[idx,:,:])\n",
    "pct = np.percentile(T, 99.9)\n",
    "T[T>pct] = pct\n",
    "plt.imshow(np.log2(T))\n",
    "plt.colorbar()\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(np.log2(hic_pred))\n",
    "plt.colorbar()\n",
    "plt.subplot(1,4,3)\n",
    "diff = (T - hic_pred)\n",
    "plt.imshow(np.log2(np.abs(diff)), cmap='seismic')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,4,4)\n",
    "diff[diff>0] = 1\n",
    "diff[diff<0] = -1\n",
    "plt.imshow((diff), cmap='seismic')\n",
    "plt.colorbar()\n",
    "\n",
    "print('T: ', np.sum(T))\n",
    "print('pred: ', np.sum(hic_pred))\n",
    "print('diff: ', np.sum(np.abs(T-hic_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, img_as_float\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import normalized_root_mse\n",
    "def mse(x, y):\n",
    "    return np.linalg.norm(x - y) #The Frobenius norm\n",
    "\n",
    "T = np.squeeze(hic_hr[idx,:,:])\n",
    "pct = np.percentile(T, 100)\n",
    "T[T>pct] = pct\n",
    "\n",
    "P = copy.copy(hic_pred)\n",
    "#P = np.squeeze(hic_hr[0,:,:])\n",
    "pct = np.percentile(P, 100)\n",
    "P[P>pct] = pct\n",
    "\n",
    "mse_noise = normalized_root_mse(T, P)\n",
    "ssim_noise = ssim(T, P, data_range=T.max() - T.min())\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 9),\n",
    "                         sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "label = 'MSE: {:.2f}, SSIM: {:.2f}'\n",
    "\n",
    "axes[0,0].imshow(np.log2(100*T), cmap='RdBu_r')\n",
    "#axes[0,0].set_xlabel(label.format(mse_noise, ssim_noise))\n",
    "axes[0,0].set_title('Original image')\n",
    "\n",
    "axes[0,1].imshow(np.log2(100*P), cmap='RdBu_r')\n",
    "#axes[0,1].set_xlabel(label.format(mse_noise, ssim_noise))\n",
    "axes[0,1].set_title('Estimate image')\n",
    "\n",
    "axes[1,0].imshow(np.log1p(100*T), cmap='RdBu_r')\n",
    "#axes[1,0].set_xlabel(label.format(mse_noise, ssim_noise))\n",
    "axes[1,0].set_title('Original image')\n",
    "\n",
    "axes[1,1].imshow(np.log1p(100*P), cmap='RdBu_r')\n",
    "axes[1,1].set_xlabel(label.format(mse_noise, ssim_noise))\n",
    "axes[1,1].set_title('Estimate image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen.save('./saved_model/gen_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model = tf.keras.models.Model(inputs=Gen.input, outputs=Gen.get_layer('Rech').output )\n",
    "intermediate_output = intermediate_layer_model.predict(test_input)\n",
    "#print(intermediate_output.shape)\n",
    "step = 2\n",
    "fig = plt.figure(figsize=(10, 10), constrained_layout=False)\n",
    "for i in range(0,128):#,intermediate_output.shape[3],step):\n",
    "    #print(i)\n",
    "    plt.subplot(16, 16, i+1)\n",
    "    plt.imshow((np.squeeze(intermediate_output[0, :, :, i])), cmap='YlOrRd')\n",
    "    plt.axis('off')\n",
    "#plt.savefig('./lvl4/image_features_log2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import glob\n",
    "anim_file = 'gcn_1.gif'\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('./lvl2/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}